{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duy/miniconda3/envs/avalanche/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCScenario object at 0x7f396d086d60>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# print to stdout\u001b[39;00m\n\u001b[1;32m     33\u001b[0m interactive_logger \u001b[39m=\u001b[39m InteractiveLogger()\n\u001b[0;32m---> 35\u001b[0m eval_plugin \u001b[39m=\u001b[39m EvaluationPlugin(\n\u001b[1;32m     36\u001b[0m     accuracy_metrics(minibatch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, experience\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     37\u001b[0m     loss_metrics(minibatch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, experience\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     38\u001b[0m     timing_metrics(epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     39\u001b[0m     cpu_usage_metrics(experience\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     40\u001b[0m     forgetting_metrics(experience\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     41\u001b[0m     StreamConfusionMatrix(num_classes\u001b[39m=\u001b[39;49mbenchmark\u001b[39m.\u001b[39;49mn_classes, save_image\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m     42\u001b[0m     disk_usage_metrics(minibatch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, experience\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     43\u001b[0m     loggers\u001b[39m=\u001b[39;49m[interactive_logger, text_logger, tb_logger],\n\u001b[1;32m     44\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[39m# CREATE THE STRATEGY INSTANCE (NAIVE)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m cl_strategy \u001b[39m=\u001b[39m Naive(\n\u001b[1;32m     49\u001b[0m     model, SGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m),\n\u001b[1;32m     50\u001b[0m     CrossEntropyLoss(), train_mb_size\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, train_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, eval_mb_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     51\u001b[0m     evaluator\u001b[39m=\u001b[39meval_plugin\n\u001b[1;32m     52\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'benchmark'"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import PermutedMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,\\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,\\\n",
    "    disk_usage_metrics, gpu_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training import EWC\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "benchmark = PermutedMNIST(n_experiences=5)\n",
    "\n",
    "\n",
    "# MODEL CREATION\n",
    "model = SimpleMLP(num_classes=benchmark.n_classes)\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns \n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(\n",
    "            minibatch=True, epoch=True, experience=True, stream=True\n",
    "        ),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        forgetting_metrics(experience=True, stream=True),\n",
    "        bwt_metrics(experience=True, stream=True),\n",
    "        loggers=[interactive_logger, tensorboard_logger],\n",
    "    )\n",
    "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
    "cl_strategy = Naive(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=500, train_epochs=1, eval_mb_size=100,\n",
    "    evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "cl_strategy = EWC(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(),  \n",
    ")\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, num_workers=4)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # eval also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream, num_workers=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('avalanche')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6140b88b016b13e67cebbf1ee2a30368da9d2346df96e8492c0f3e75c904405a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
